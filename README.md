# regul-energia-lakehouse


# üåê Projeto CKAN API ‚Äì Continuidade e Compensa√ß√£o

Este projeto tem como objetivo automatizar a **extra√ß√£o, padroniza√ß√£o e consolida√ß√£o de dados p√∫blicos** provenientes do **CKAN (Comprehensive Knowledge Archive Network)**, com foco espec√≠fico nas tem√°ticas de **continuidade e compensa√ß√£o** de benef√≠cios e pol√≠ticas sociais.

A solu√ß√£o foi desenvolvida dentro do ecossistema **Databricks**, aproveitando os recursos nativos de **orquestra√ß√£o (Workflows/Jobs)**, **Delta Lake**, **PySpark** e **Unity Catalog** para garantir governan√ßa, rastreabilidade e performance.

---

## üß© Contexto

O CKAN √© uma plataforma aberta amplamente utilizada por √≥rg√£os p√∫blicos para **publicar e gerenciar dados governamentais**.  
Neste projeto, os datasets extra√≠dos referem-se a registros administrativos e operacionais ligados √† **execu√ß√£o de programas sociais**, especialmente o **BPC (Benef√≠cio de Presta√ß√£o Continuada)**.

A an√°lise de **continuidade** e **compensa√ß√£o** busca identificar:
- **Continuidade** ‚Üí se um benefici√°rio manteve o recebimento do benef√≠cio ao longo do tempo, avaliando eventuais interrup√ß√µes administrativas;
- **Compensa√ß√£o** ‚Üí casos em que h√° sobreposi√ß√£o ou substitui√ß√£o de pagamentos (ex.: valores restitu√≠dos ou compensados entre per√≠odos).

Essas informa√ß√µes s√£o fundamentais para:
- Monitorar **regularidade dos pagamentos**;
- Detectar **falhas ou duplicidades** entre bases;
- Apoiar **tomadas de decis√£o** e auditorias internas.

---

## ‚öôÔ∏è Arquitetura e Tecnologias

A pipeline segue o modelo de arquitetura **Medallion (Bronze ‚Üí Silver ‚Üí Gold)** dentro do Databricks, com **Jobs** controlando o fluxo de execu√ß√£o.

| Camada | Descri√ß√£o | Tecnologias |
|---------|------------|-------------|
| **Bronze** | Ingest√£o bruta dos dados extra√≠dos da API CKAN. | `Python`, `Requests`, `Databricks Jobs` |
| **Silver** | Padroniza√ß√£o, limpeza, enriquecimento e reconcilia√ß√£o de inconsist√™ncias. | `PySpark`, `Delta Lake` |
| **Gold** | Modelagem anal√≠tica final (tabelas fato e dimens√£o, m√©tricas de continuidade e compensa√ß√£o). | `SQL`, `Power BI`, `Unity Catalog` |

---

## üß† Orquestra√ß√£o no Databricks Workflows

A execu√ß√£o do pipeline √© feita via **Databricks Workflows (Jobs)** ‚Äî uma ferramenta nativa de orquestra√ß√£o, agendamento e monitoramento.

### üîÅ Estrutura do Job

**Job: `ckan_continuidade_compensacao`**

| Task | Descri√ß√£o | Tipo | Depend√™ncia |
|------|------------|------|--------------|
| **1. Extra√ß√£o CKAN** | Conecta √† API CKAN, baixa os datasets e salva na camada Bronze. | Notebook Python | ‚Äî |
| **2. Transforma√ß√£o / Compensa√ß√£o** | Aplica regras de continuidade e compensa√ß√£o (PySpark). | Notebook PySpark | Task 1 |
| **3. Publica√ß√£o Final** | Atualiza tabelas Gold e exp√µe m√©tricas anal√≠ticas. | Notebook SQL | Task 2 |

Cada task roda em **clusters otimizados**, com controle de versionamento e alertas configurados para falhas ou execu√ß√µes parciais.

### üìÖ Agendamentos e Alertas

- **Agendamento**: di√°rio √†s 02h00 (ajust√°vel conforme atualiza√ß√£o da API CKAN)  
- **Retries autom√°ticos** em caso de erro de rede na ingest√£o  
- **Notifica√ß√£o via e-mail ou webhook** quando o job falhar ou concluir com warnings  

---

```text
[runjobs.py] ‚îÄ‚îÄchama‚îÄ‚îÄ>  [Wrappers]
                             ‚îÇ
                             ‚îú‚îÄ‚îÄ load_continuidades() ‚îÄ‚îÄ‚ñ∫ baixar_e_carregar(READ_CONT, "stg_continuidades_2020_2025", filtros)
                             ‚îú‚îÄ‚îÄ load_compensacoes() ‚îÄ‚îÄ‚îÄ‚ñ∫ baixar_e_carregar(READ_COMP, "stg_compensacoes_2020_2025", filtros)
                             ‚îî‚îÄ‚îÄ load_limites() ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ baixar_e_carregar(READ_LIMIT, "stg_limites")

                                   ‚îÇ
                                   ‚ñº
                           [Fun√ß√£o CORE]
                        baixar_e_carregar(...)
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  1) Monta request CKAN (resource_id, limit, offset, filters)  ‚îÇ
      ‚îÇ  2) Faz pagina√ß√£o (while offset += batch)                     ‚îÇ
      ‚îÇ  3) Converte p/ DataFrame + limpeza b√°sica (trim, tipos)      ‚îÇ
      ‚îÇ  4) Grava em Postgres (to_sql append, chunks)                 ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚ñº                                                ‚ñº
    [API CKAN / dados abertos]                       [PostgreSQL / Staging]
   (datastore_search / _sql)                        stg_continuidades_2020_2025
                                                    stg_compensacoes_2020_2025
                                                          stg_limites
```

## Passos
1. **Banco**: criar DB `case_equatorial` e schemas `raw`, `stg`, `core`.
2. **Ingest√£o**: rodar scripts em `/src/ingestion/` (CKAN ‚Üí `stg_*`).
3. **Transform**: `dbt init`, configurar profile Postgres, `dbt deps`, `dbt run`, `dbt test`.
4. **Observabilidade**: `edr report` (Elementary) para gerar relat√≥rio HTML de sa√∫de.
5. **(Opcional)**: Painel Streamlit para m√©tricas de qualidade (freshness, volumes, falhas).

## Qualidade & Observabilidade (o que √© checado)
- **Conformidade**: tipos/valores v√°lidos (`indicador ‚àà {DEC,FEC}`, `mes ‚àà 1..12`, `ano ‚àà 2020..2025`)
- **Completude**: % nulos em campos cr√≠ticos; meses faltantes por distribuidora
- **Consist√™ncia**: chaves √∫nicas `(ide_conjunto, ano, mes, indicador)`; FK para `dim_conjunto`
- **Acur√°cia (pragm√°tica)**: faixas plaus√≠veis (FEC ‚â§ 50; DEC ‚â• 0)
- **Pontualidade (Freshness)**: `MAX(dat_geracao)` dentro do SLA mensal
- **Volume**: linhas por m√™s comparado ao hist√≥rico

## Comandos √∫teis
```bash
# instalar pacotes
pip install -U pandas requests sqlalchemy psycopg2-binary python-dotenv dbt-postgres elementary-data

# rodar dbt
dbt deps
dbt run
dbt test

# relat√≥rio elementary
edr report

```

## Estrutura do Reposit√≥rio

```text
/docs/            # vis√£o, diagramas, decis√µes de arquitetura
/src/
  ingestion/      # scripts de ingest√£o (CKAN -> staging no Postgres)
  quality/        # valida√ß√µes de data quality (ex: Pandera / Great Expectations)
  transforms/     # SQL: dimens√µes, fatos, views (camada core)
  analytics/      # notebooks e an√°lises explorat√≥rias
/app/             # app (ex: Streamlit) e guias de visualiza√ß√£o (Power BI)
/infra/           # infraestrutura (docker-compose, configs, .env.example)
README.md         # vis√£o geral do projeto
LICENSE           # licen√ßa do reposit√≥rio

