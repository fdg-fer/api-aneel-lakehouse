# üåê Projeto CKAN API ‚Äì Continuidade e Compensa√ß√£o

## ‚öôÔ∏è Contexto

A **ANEEL (Ag√™ncia Nacional de Energia El√©trica)** disponibiliza mensalmente, em seu portal de dados abertos ([dados.aneel.gov.br](https://dados.aneel.gov.br/)), informa√ß√µes sobre a **qualidade do fornecimento de energia el√©trica**, enviadas por todas as **distribuidoras do pa√≠s**.

Os principais conjuntos de dados tratados neste projeto s√£o:

| Indicador | Nome | Descri√ß√£o | Unidade |
|------------|------|------------|----------|
| **DEC** | Dura√ß√£o Equivalente de Interrup√ß√£o por Unidade Consumidora | Mede o tempo m√©dio (em horas) que os consumidores ficaram sem energia em determinado per√≠odo. | horas |
| **FEC** | Frequ√™ncia Equivalente de Interrup√ß√£o por Unidade Consumidora | Mede o n√∫mero m√©dio de interrup√ß√µes no fornecimento de energia por unidade consumidora. | vezes |
| **Compensa√ß√£o** | Compensa√ß√£o Financeira Autom√°tica | Representa os valores (em R$) creditados aos consumidores quando os limites de continuidade (DEC/FEC) s√£o ultrapassados. | reais |

Os indicadores **DEC** e **FEC** comp√µem o conjunto de **indicadores de continuidade do fornecimento**, enquanto o dado de **compensa√ß√£o** reflete o **impacto financeiro regulat√≥rio** dessas viola√ß√µes, conforme definido nos **Procedimentos de Distribui√ß√£o (PRODIST) ‚Äì M√≥dulo 8** da ANEEL.

---

## ‚öôÔ∏è Arquitetura e Tecnologias

A pipeline segue o modelo de arquitetura **Medallion (Bronze ‚Üí Silver ‚Üí Gold)** dentro do Databricks, com **Jobs** controlando o fluxo de execu√ß√£o.

| Camada | Descri√ß√£o | Tecnologias |
|---------|------------|-------------|
| **Bronze** | Ingest√£o bruta dos dados extra√≠dos da API CKAN. | `Python`, `Requests`, `Databricks Jobs` |
| **Silver** | Padroniza√ß√£o, limpeza, enriquecimento e reconcilia√ß√£o de inconsist√™ncias. | `PySpark`, `Delta Lake` |
| **Gold** | Modelagem anal√≠tica final (tabelas fato e dimens√£o, m√©tricas de continuidade e compensa√ß√£o). | `SQL`, `Power BI`, `Unity Catalog` |

---

## üß† Orquestra√ß√£o no Databricks Workflows

A execu√ß√£o do pipeline √© feita via **Databricks Workflows (Jobs)** ‚Äî uma ferramenta nativa de orquestra√ß√£o, agendamento e monitoramento.

### üîÅ Estrutura do Job

**Job: `ckan_continuidade_compensacao`**

| Task | Descri√ß√£o | Tipo | Depend√™ncia |
|------|------------|------|--------------|
| **1. Extra√ß√£o CKAN** | Conecta √† API CKAN, baixa os datasets e salva na camada Bronze. | Notebook Python | ‚Äî |
| **2. Transforma√ß√£o / Compensa√ß√£o** | Aplica regras de continuidade e compensa√ß√£o (PySpark). | Notebook PySpark | Task 1 |
| **3. Publica√ß√£o Final** | Atualiza tabelas Gold e exp√µe m√©tricas anal√≠ticas. | Notebook SQL | Task 2 |

Cada task roda em **clusters otimizados**, com controle de versionamento e alertas configurados para falhas ou execu√ß√µes parciais.

### üìÖ Agendamentos e Alertas

- **Agendamento**: di√°rio √†s 02h00 (ajust√°vel conforme atualiza√ß√£o da API CKAN)  
- **Retries autom√°ticos** em caso de erro de rede na ingest√£o  
- **Notifica√ß√£o via e-mail ou webhook** quando o job falhar ou concluir com warnings  

---

```text
[runjobs.py] ‚îÄ‚îÄchama‚îÄ‚îÄ>  [Wrappers]
                             ‚îÇ
                             ‚îú‚îÄ‚îÄ load_continuidades() ‚îÄ‚îÄ‚ñ∫ baixar_e_carregar(READ_CONT, "stg_continuidades_2020_2025", filtros)
                             ‚îú‚îÄ‚îÄ load_compensacoes() ‚îÄ‚îÄ‚îÄ‚ñ∫ baixar_e_carregar(READ_COMP, "stg_compensacoes_2020_2025", filtros)
                             ‚îî‚îÄ‚îÄ load_limites() ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ baixar_e_carregar(READ_LIMIT, "stg_limites")

                                   ‚îÇ
                                   ‚ñº
                           [Fun√ß√£o CORE]
                        baixar_e_carregar(...)
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  1) Monta request CKAN (resource_id, limit, offset, filters)  ‚îÇ
      ‚îÇ  2) Faz pagina√ß√£o (while offset += batch)                     ‚îÇ
      ‚îÇ  3) Converte p/ DataFrame + limpeza b√°sica (trim, tipos)      ‚îÇ
      ‚îÇ  4) Grava em Postgres (to_sql append, chunks)                 ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚ñº                                                ‚ñº
    [API CKAN / dados abertos]                       [PostgreSQL / Staging]
   (datastore_search / _sql)                        stg_continuidades_2020_2025
                                                    stg_compensacoes_2020_2025
                                                          stg_limites
```

## Passos
1. **Banco**: criar DB `case_equatorial` e schemas `raw`, `stg`, `core`.
2. **Ingest√£o**: rodar scripts em `/src/ingestion/` (CKAN ‚Üí `stg_*`).
3. **Transform**: `dbt init`, configurar profile Postgres, `dbt deps`, `dbt run`, `dbt test`.
4. **Observabilidade**: `edr report` (Elementary) para gerar relat√≥rio HTML de sa√∫de.
5. **(Opcional)**: Painel Streamlit para m√©tricas de qualidade (freshness, volumes, falhas).

## Qualidade & Observabilidade (o que √© checado)
- **Conformidade**: tipos/valores v√°lidos (`indicador ‚àà {DEC,FEC}`, `mes ‚àà 1..12`, `ano ‚àà 2020..2025`)
- **Completude**: % nulos em campos cr√≠ticos; meses faltantes por distribuidora
- **Consist√™ncia**: chaves √∫nicas `(ide_conjunto, ano, mes, indicador)`; FK para `dim_conjunto`
- **Acur√°cia (pragm√°tica)**: faixas plaus√≠veis (FEC ‚â§ 50; DEC ‚â• 0)
- **Pontualidade (Freshness)**: `MAX(dat_geracao)` dentro do SLA mensal
- **Volume**: linhas por m√™s comparado ao hist√≥rico

## Comandos √∫teis
```bash
# instalar pacotes
pip install -U pandas requests sqlalchemy psycopg2-binary python-dotenv dbt-postgres elementary-data

# rodar dbt
dbt deps
dbt run
dbt test

# relat√≥rio elementary
edr report

```

## Estrutura do Reposit√≥rio

```text
/docs/            # vis√£o, diagramas, decis√µes de arquitetura
/src/
  ingestion/      # scripts de ingest√£o (CKAN -> staging no Postgres)
  quality/        # valida√ß√µes de data quality (ex: Pandera / Great Expectations)
  transforms/     # SQL: dimens√µes, fatos, views (camada core)
  analytics/      # notebooks e an√°lises explorat√≥rias
/app/             # app (ex: Streamlit) e guias de visualiza√ß√£o (Power BI)
/infra/           # infraestrutura (docker-compose, configs, .env.example)
README.md         # vis√£o geral do projeto
LICENSE           # licen√ßa do reposit√≥rio

